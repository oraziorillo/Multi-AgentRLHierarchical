{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import pprint\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ddpg.ddpg import DDPGTrainer\n",
    "\n",
    "from envs.particle_rllib.environment import ParticleEnv\n",
    "\n",
    "from callbacks import CustomCallbacks\n",
    "from logger import info_logger, results_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates the environment\n",
    "def create_env_fn(env_context=None):\n",
    "    return ParticleEnv(n_listeners=n_listeners, \n",
    "                       n_landmarks=n_landmarks,\n",
    "                       render_enable=render_enable)\n",
    "\n",
    "# Function that maps a policy to its agent id\n",
    "def policy_mapping_fn(agent_id):\n",
    "    if agent_id.startswith('manager'):\n",
    "        return \"manager_policy\"\n",
    "    else:\n",
    "        return \"worker_policy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-executive",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining parameters\n",
    "pretraining_n_epochs = 7\n",
    "\n",
    "# training parameters\n",
    "training_n_epochs = 100\n",
    "\n",
    "# common parameters\n",
    "training_algo = \"DDPG\"\n",
    "env_name = \"ParticleManagerListeners\"\n",
    "n_episodes = 70 # number of episodes in one epoch\n",
    "n_steps = 25 # number of steps in one episode\n",
    "learning_rate = 0.001 \n",
    "tau = 0.01 # for updating the target network\n",
    "gamma = 0.75 # discount factor\n",
    "replay_buffer_size = 10000000\n",
    "batch_size = 1024\n",
    "hidden_layers = [256, 256]\n",
    "\n",
    "# environment config\n",
    "n_listeners = 1 \n",
    "n_landmarks = 12\n",
    "render_enable = False\n",
    "\n",
    "# early stop training parameters\n",
    "early_stop_enable = True # set to True to enable early stopping based on conditions defined below\n",
    "min_n_epochs = 0.5 * training_n_epochs # minimum number of epochs to enable early stopping\n",
    "min_rel_delta_reward = 0.01  # minimum acceptable variation of the reward\n",
    "cut_epochs = 5 # minimun number of epochs without significant variation of the reward to stop the training loop\n",
    "\n",
    "# other settings\n",
    "savedata_dir = './savedata/'\n",
    "checkpoint_dir = './checkpoints/' # checkpoints directory\n",
    "checkpoint_interval = 2 # number of trainings after which a checkpoint is set\n",
    "restore_checkpoint_n = 10\n",
    "\n",
    "# Create checkpoint directory\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "# Create savedata directory\n",
    "if not os.path.exists(savedata_dir):\n",
    "    os.makedirs(savedata_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-parameter",
   "metadata": {},
   "source": [
    "## Environment and trainers configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and register the environment\n",
    "register_env(env_name, create_env_fn)\n",
    "env = create_env_fn()\n",
    "\n",
    "action_spaces = env.action_space\n",
    "observation_spaces = env.observation_space\n",
    "\n",
    "# According to environment implementation, there exists a different action space and observation space for each agent, \n",
    "# action_space[0] (resp. observations_space[0]) is allocated for the manager, while the others are allocated for the workers\n",
    "manager_action_space = action_spaces[0]\n",
    "manager_observation_space = observation_spaces[0]\n",
    "worker_action_space = action_spaces[1]\n",
    "worker_observation_space = observation_spaces[1]\n",
    "\n",
    "policies = {\n",
    "    \"manager_policy\": (None, manager_observation_space, manager_action_space, {}),\n",
    "    \"worker_policy\": (None, worker_observation_space, worker_action_space, {})\n",
    "    }\n",
    "\n",
    "pretraining_config = {\n",
    "    \"lr\": learning_rate,\n",
    "    \"tau\": tau,\n",
    "    \"gamma\": gamma,\n",
    "    \"horizon\": n_steps,\n",
    "    \"actor_hiddens\": hidden_layers,\n",
    "    \"critic_hiddens\": hidden_layers,\n",
    "    \"buffer_size\": replay_buffer_size,\n",
    "    \"train_batch_size\": batch_size,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "        \"policies_to_train\": [\"worker_policy\"]\n",
    "    },\n",
    "    \"callbacks\": CustomCallbacks,\n",
    "    \"log_level\": \"ERROR\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-horizon",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    curr_epoch = 1 # Current epoch\n",
    "    checkpoint_counter = 0 # Counter to know when to save a new checkpoint\n",
    "\n",
    "    # Initialize Ray\n",
    "    ray.shutdown()\n",
    "    ray.init()\n",
    "    \n",
    "    # Initialize trainer, start with pre-training configuration\n",
    "    trainer = DDPGTrainer(env=env_name, \n",
    "                          config=pretraining_config)\n",
    "    \n",
    "    # Restore a checkpoint\n",
    "    if(restore_checkpoint_n != 0):\n",
    "        trainer.restore(checkpoint_dir + 'checkpoint_{n}/checkpoint-{n}'.format(n=restore_checkpoint_n))\n",
    "        curr_epoch = trainer._episodes_total // n_episodes + 1\n",
    "\n",
    "        if curr_epoch > pretraining_n_epochs:\n",
    "            training_config = trainer.get_config()\n",
    "            training_config['multiagent']['policies_to_train'] = list(policies.keys())\n",
    "            trainer._setup(training_config)\n",
    "\n",
    "        info_logger.info(\"Restored checkpoint {}\".format(restore_checkpoint_n))\n",
    "    else:\n",
    "        info_logger.info(\"Initializing pre-training mode\")\n",
    "\n",
    "    countdown_enabled = False # flag for early stopping\n",
    "    countdown = cut_epochs\n",
    "\n",
    "    # Print the current configuration\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    print(\"Current configiguration\\n-----------------------\")\n",
    "    pp.pprint(trainer.get_config())\n",
    "    print(\"-----------------------\\n\")\n",
    "\n",
    "    while curr_epoch <= training_n_epochs:\n",
    "\n",
    "        # loop for training_n_epochs\n",
    "        \n",
    "        info_logger.info(\"Current epoch: {}\".format(curr_epoch))\n",
    "\n",
    "        # initialize iteration data saving log\n",
    "        savedata_file_name = '{}-epoch.csv'.format(curr_epoch)\n",
    "        savedata_file_path = savedata_dir + \"/\" + savedata_file_name\n",
    "        savedata_columns = [\"episodes_total\",\"episode_len_mean\", \"worker_reward_mean\", \"manager_reward_mean\", \"prob_correct_goal\"]\n",
    "        savedata = pd.DataFrame(columns=savedata_columns)\n",
    "    \n",
    "        episode_mean_rewards = [] # mean reward of the episode in time\n",
    "\n",
    "        # after pretraining_n_epochs epochs, reset the configuration to the training one\n",
    "        if curr_epoch == pretraining_n_epochs + 1:\n",
    "            training_config = trainer.get_config()\n",
    "            training_config['multiagent']['policies_to_train'] = list(policies.keys())\n",
    "            trainer._setup(training_config)\n",
    "            info_logger.info(\"Switch to training mode\")\n",
    "            print(\"Training configiguration\\n-----------------------\")\n",
    "            pp.pprint(trainer.get_config())\n",
    "            print(\"-----------------------\\n\")\n",
    "\n",
    "        curr_episode = 1 # Current episode\n",
    "\n",
    "        while curr_episode <= n_episodes * curr_epoch:\n",
    "\n",
    "            # loop for n_episodes\n",
    "\n",
    "            result = trainer.train()\n",
    "            curr_episode = result['episodes_total']\n",
    "            episode_mean_reward = result['episode_reward_mean']\n",
    "            episode_mean_len = result['episode_len_mean']\n",
    "            prob_correct_goal = result['custom_metrics']['prob_correct_goal_mean']\n",
    "\n",
    "            print(pretty_print(result))\n",
    "            episode_mean_rewards.append(episode_mean_reward)\n",
    "            plt.plot(episode_mean_rewards)\n",
    "            plt.show()\n",
    "            \n",
    "            checkpoint_counter += 1\n",
    "            # save a checkpoint every checkpoint_interval trains\n",
    "            if(checkpoint_counter == checkpoint_interval):\n",
    "                trainer.save(checkpoint_dir)\n",
    "                info_logger.info(\"Checkpoint saved (iteration {})\".format(result['training_iteration']))\n",
    "                checkpoint_counter = 0\n",
    "\n",
    "            # update the log \n",
    "            training_data = []\n",
    "            training_data.append(curr_episode) # first entry is the total number of episodes\n",
    "            training_data.append(episode_mean_reward) # second entry is the mean episode length\n",
    "            training_data += [result['policy_reward_mean'][policy_name] \n",
    "                                   for policy_name in policies.keys()]  # other entries are mean policy rewards\n",
    "            training_data.append(prob_correct_goal)\n",
    "            training_data_df = pd.DataFrame([training_data], columns=savedata_columns)\n",
    "            savedata = savedata.append(training_data_df, ignore_index=True)\n",
    "\n",
    "        # print results on file\n",
    "        savedata.to_csv(savedata_file_path)\n",
    "\n",
    "        # compute epoch's results\n",
    "        curr_epoch_mean_reward = result['episode_reward_mean']\n",
    "        curr_epoch_prob_correct_goal = result['custom_metrics']['prob_correct_goal_mean']\n",
    "        \n",
    "        results_logger.info(\"Epoch: {}\".format(curr_epoch))\n",
    "        results_logger.info(\"\\tmean reward = {}\".format(curr_epoch_mean_reward))\n",
    "        results_logger.info(\"\\tprobability of correct goal = {}\".format(curr_epoch_prob_correct_goal))\n",
    "\n",
    "        # check early stopping conditions\n",
    "        if(early_stop_enable):\n",
    "\n",
    "            # 1st stopping criterion: the minimum number of epochs to enable early stop has been reached\n",
    "            if(curr_epoch >= min_n_epochs):\n",
    "\n",
    "                if(countdown_enabled):  \n",
    "\n",
    "                    # compute the minimum acceptable variation of the reward with respect to the previous episode\n",
    "                    min_delta_reward = min_rel_delta_reward * prev_epoch_mean_reward \n",
    "                    # compute the actual variation of the reward with respect to the previous episode\n",
    "                    delta_reward = abs(curr_epoch_mean_reward - prev_epoch_mean_reward)\n",
    "\n",
    "                    # check that the variation of the reward is significant\n",
    "                    if(delta_reward > min_delta_reward):\n",
    "\n",
    "                        # reset the countdown\n",
    "                        countdown = cut_epochs\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        countdown -= 1\n",
    "\n",
    "                        # 2nd stopping criterion: there has not been any significant variation for more than cut_epochs episodes\n",
    "                        if(countdown == 0):\n",
    "                            break\n",
    "\n",
    "                else: \n",
    "                    # this branch is executed the first time the program reaches the minimum number of episodes only, start the countdown\n",
    "                    countdown_enabled = True\n",
    "\n",
    "                prev_epoch_mean_reward = curr_epoch_mean_reward\n",
    "                \n",
    "        curr_epoch +=1\n",
    "\n",
    "finally:\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
