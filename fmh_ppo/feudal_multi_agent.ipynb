{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "feudal_multi_agent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNh216CCm9ID"
      },
      "source": [
        "# If you are running on Google Colab, please mount the drive uncommenting below\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/semester_project_experiments/fmh/\")"
      ],
      "id": "JNh216CCm9ID",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olgx1Z3_HHqo"
      },
      "source": [
        "# # If you are running on Google Colab, please install TensorFlow 2.0 by uncommenting below\n",
        "\n",
        "# try:\n",
        "#   # %tensorflow_version only exists in Colab.\n",
        "#   %tensorflow_version 2.x\n",
        "# except Exception:\n",
        "#   pass"
      ],
      "id": "olgx1Z3_HHqo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZgLY6rMHGN3"
      },
      "source": [
        "# # If you are running on Google Colab, uncomment below to install the necessary dependencies \n",
        "# # before running the experiment, then comment it again\n",
        "\n",
        "# print(\"Setting up colab environment\")\n",
        "# !pip uninstall -y -q pyarrow\n",
        "# !pip install -q -U ray[tune]\n",
        "# !pip install -q ray[debug]\n",
        "\n",
        "# !pip install lz4\n",
        "# !pip install gputil\n",
        "\n",
        "# # A hack to force the runtime to restart, needed to include the above dependencies.\n",
        "# print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n",
        "# import os\n",
        "# os._exit(0)"
      ],
      "id": "hZgLY6rMHGN3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sporting-dover"
      },
      "source": [
        "import json\n",
        "import datetime\n",
        "import pprint\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.registry import register_env\n",
        "from ray.tune.logger import pretty_print\n",
        "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
        "\n",
        "from envs.particle_rllib.environment import ParticleEnv\n",
        "\n",
        "from callbacks import CustomCallbacks\n",
        "from logger import info_logger, results_logger"
      ],
      "id": "sporting-dover",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8qG4UBexSNm"
      },
      "source": [
        "## Helper functions"
      ],
      "id": "x8qG4UBexSNm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extra-image"
      },
      "source": [
        "# Function that creates the environment\n",
        "def create_env_fn(env_context=None):\n",
        "    return ParticleEnv(n_listeners=n_listeners, \n",
        "                       n_landmarks=n_landmarks,\n",
        "                       render_enable=render_enable)\n",
        "\n",
        "# Function that maps a policy to its agent id\n",
        "def policy_mapping_fn(agent_id):\n",
        "    if agent_id.startswith('manager'):\n",
        "        return \"manager_policy\"\n",
        "    else:\n",
        "        return \"worker_policy\"\n",
        "\n",
        "# Functions to write and read the checkpoint number to epoch number dictionary \n",
        "def write_epoch_by_checkpoint(epoch_by_checkpoint):\n",
        "    with open('temp/epoch-by-checkpoint.json', 'w') as f:\n",
        "        json.dump(epoch_by_checkpoint, f)\n",
        "\n",
        "def read_epoch_by_checkpoint():\n",
        "    with open('temp/epoch-by-checkpoint.json') as f:\n",
        "        epoch_by_checkpoint = json.load(f)\n",
        "    return epoch_by_checkpoint "
      ],
      "id": "extra-image",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "retired-executive"
      },
      "source": [
        "## Parameters"
      ],
      "id": "retired-executive"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boring-giant"
      },
      "source": [
        "# pretraining parameters\n",
        "pretraining_n_epochs = 10\n",
        "\n",
        "# training parameters\n",
        "training_n_epochs = 100\n",
        "\n",
        "# common parameters\n",
        "training_algo = \"PPO\"\n",
        "env_name = \"ParticleManagerListeners\"\n",
        "n_episodes = 1000 # number of episodes in one epoch\n",
        "n_steps = 25 # number of steps in one episode\n",
        "learning_rate = 0.001 \n",
        "tau = 0.01 # for updating the target network\n",
        "gamma = 0.75 # discount factor\n",
        "replay_buffer_size = 10000000\n",
        "batch_size = 1024\n",
        "hidden_layers = [256, 256]\n",
        "\n",
        "# environment config parameters\n",
        "n_listeners = 1 \n",
        "n_landmarks = 12\n",
        "render_enable = False\n",
        "\n",
        "# convergence parameters\n",
        "window_size = 5 # size of the sliding window \n",
        "min_rel_delta_reward = 0.02  # minimum acceptable variation of the reward\n",
        "\n",
        "# savedata filepath\n",
        "savedata_dir = './savedata/'\n",
        "\n",
        "# checkpoint parameters \n",
        "checkpoint_interval = 20 # number of trainings after which a checkpoint is set\n",
        "checkpoint_mode = 'pretraining' # mode of the checkpoint to restore\n",
        "restore_checkpoint_n = 0\n",
        "    \n",
        "# Create savedata directory\n",
        "if not os.path.exists(savedata_dir):\n",
        "    os.makedirs(savedata_dir)"
      ],
      "id": "boring-giant",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOJbJTdmRBZm"
      },
      "source": [
        "## Environment spaces"
      ],
      "id": "KOJbJTdmRBZm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q_xbbQGQ1Kx"
      },
      "source": [
        "env = create_env_fn()\n",
        "\n",
        "# According to environment implementation, there exists a different action space and observation space for each agent, \n",
        "# action_space[0] (resp. observations_space[0]) is allocated for the manager, while the others are allocated for the workers\n",
        "manager_action_space = env.action_space[0]\n",
        "manager_observation_space = env.observation_space[0]\n",
        "worker_action_space = env.action_space[1]\n",
        "worker_observation_space = env.observation_space[1]"
      ],
      "id": "4q_xbbQGQ1Kx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "valuable-parameter"
      },
      "source": [
        "## Trainers configuration"
      ],
      "id": "valuable-parameter"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unavailable-boundary"
      },
      "source": [
        "policies = {\n",
        "    \"manager_policy\": (None, manager_observation_space, manager_action_space, {}),\n",
        "    \"worker_policy\": (None, worker_observation_space, worker_action_space, {})\n",
        "    }\n",
        "\n",
        "pretraining_config = {\n",
        "    \"num_workers\": 2,\n",
        "    \"lr\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "    \"horizon\": n_steps,\n",
        "    \"train_batch_size\": batch_size,\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "        \"policies_to_train\": [\"worker_policy\"]\n",
        "    },\n",
        "    \"callbacks\": CustomCallbacks,\n",
        "    \"no_done_at_end\": True,\n",
        "    \"log_level\": \"ERROR\"\n",
        "}\n",
        "\n",
        "training_config = {\n",
        "    \"num_workers\": 2,\n",
        "    \"lr\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "    \"horizon\": n_steps,\n",
        "    \"train_batch_size\": batch_size,\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "        \"policies_to_train\": list(policies.keys())\n",
        "    },\n",
        "    \"callbacks\": CustomCallbacks,\n",
        "    \"no_done_at_end\": True,\n",
        "    \"log_level\": \"ERROR\"\n",
        "}"
      ],
      "id": "unavailable-boundary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp53vGPy5fB9"
      },
      "source": [
        "## Initialize the environment, the trainer and the checkpoint folder/parameters"
      ],
      "id": "cp53vGPy5fB9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQNLWa685lq6"
      },
      "source": [
        "# Initialize and register the environment\n",
        "register_env(env_name, create_env_fn)\n",
        "\n",
        "# Initialize Ray\n",
        "ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "if restore_checkpoint_n != 0:\n",
        "    # Read the number of the epoch of the checkpoint to restore\n",
        "    epoch_by_checkpoint = read_epoch_by_checkpoint()\n",
        "    curr_epoch = epoch_by_checkpoint[checkpoint_mode][str(restore_checkpoint_n)]\n",
        "\n",
        "    if curr_epoch > pretraining_n_epochs:\n",
        "\n",
        "        # Initialize the trainer with the training configuration\n",
        "        mode = 'training'\n",
        "        trainer = PPOTrainer(env=env_name, config=training_config)\n",
        "      \n",
        "    else:\n",
        "\n",
        "        # Initialize the trainer with the pre-training configuration\n",
        "        mode = 'pretraining'\n",
        "        trainer = PPOTrainer(env=env_name, config=pretraining_config)\n",
        "    \n",
        "    checkpoint_dir = './checkpoints/' + mode + '/' # checkpoints directory\n",
        "    # Restore the simulation from the checkpoint\n",
        "    trainer.restore(checkpoint_dir + 'checkpoint_{n}/checkpoint-{n}'.format(n=restore_checkpoint_n))\n",
        "    info_logger.info(\"Restored checkpoint {}\".format(restore_checkpoint_n))\n",
        "\n",
        "else:\n",
        "    # Initialize the trainer with the pre-training configuration and the parameters to start a simulation from scratch \n",
        "    mode = 'pretraining'\n",
        "    curr_epoch = 1 \n",
        "    epoch_by_checkpoint = {'pretraining': {}, 'training': {}}\n",
        "    trainer = PPOTrainer(env=env_name, config=pretraining_config)\n",
        "\n",
        "    checkpoint_dir = './checkpoints/' + mode + '/' # checkpoints directory\n",
        "    # Create the checkpoint directory\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    info_logger.info(\"Initializing with pre-training mode\")\n",
        "\n",
        "# Print the current configuration\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "print(\"Current configiguration\\n-----------------------\")\n",
        "pp.pprint(trainer.get_config())\n",
        "print(\"-----------------------\\n\")"
      ],
      "id": "AQNLWa685lq6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "indonesian-horizon"
      },
      "source": [
        "## Simulation loop"
      ],
      "id": "indonesian-horizon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collective-therapy"
      },
      "source": [
        "try:\n",
        "    convergence = False\n",
        "    convergence_counter = 0 \n",
        "    checkpoint_counter = 0 # to know when it's time to save a new checkpoint\n",
        "    epoch_mean_rewards = [] # mean reward of the epoch in time\n",
        "\n",
        "    while curr_epoch <= training_n_epochs:\n",
        "\n",
        "        # loop for training_n_epochs\n",
        "        \n",
        "        info_logger.info(\"Current epoch: {}\".format(curr_epoch))\n",
        "\n",
        "        manager_mean_rewards = [] # mean reward of the training iteration in time\n",
        "        prob_correct_goal_means = [] # mean probability of communicating the correct goal\n",
        "\n",
        "        # after pretraining_n_epochs epochs, switch to training mode\n",
        "        if curr_epoch == pretraining_n_epochs + 1 and mode == 'pretraining':\n",
        "\n",
        "            info_logger.info(\"Switching to training mode\")\n",
        "            mode = 'training'\n",
        "            checkpoint_dir = './checkpoints/' + mode + '/'\n",
        "\n",
        "            # create checkpoint directory if it doesn't exist\n",
        "            if not os.path.exists(checkpoint_dir):\n",
        "                os.makedirs(checkpoint_dir)\n",
        "\n",
        "            checkpoint_counter = 0\n",
        "\n",
        "            # change the trainer configuration but keep the old weights\n",
        "            model_weights = trainer.get_weights()\n",
        "            trainer.cleanup()\n",
        "            trainer = PPOTrainer(env=env_name, config=training_config)\n",
        "            trainer.set_weights(model_weights)\n",
        "\n",
        "            print(\"Training configiguration\\n-----------------------\")\n",
        "            pp.pprint(trainer.get_config())\n",
        "            print(\"-----------------------\\n\")\n",
        "\n",
        "        # initialize iteration data saving log\n",
        "        savedata_file_name = '{}-epoch.csv'.format(curr_epoch)\n",
        "        savedata_file_path = savedata_dir + \"/\" + savedata_file_name\n",
        "        savedata_columns = [\"episodes_total\",\"episode_len_mean\", \"worker_reward_mean\", \"manager_reward_mean\", \"prob_correct_goal\"]\n",
        "        savedata = pd.DataFrame(columns=savedata_columns)\n",
        "    \n",
        "        elapsed_episodes = 0 # Current episode\n",
        "\n",
        "        # compute the episode in which the epoch will be over\n",
        "        if mode == 'pretraining':\n",
        "            # if pretraining mode, standard computation\n",
        "            end_of_epoch = n_episodes * curr_epoch\n",
        "        else:\n",
        "            # if training mode, we subtract the number of episodes of pretraining because the trainer is reset after the pretraining\n",
        "            end_of_epoch = n_episodes * (curr_epoch - pretraining_n_epochs)\n",
        "\n",
        "        while elapsed_episodes <= end_of_epoch:\n",
        "\n",
        "            # loop for n_episodes\n",
        "\n",
        "            result = trainer.train()\n",
        "            elapsed_episodes = result['episodes_total']\n",
        "            episode_mean_reward = result['episode_reward_mean']\n",
        "            manager_mean_reward = result['policy_reward_mean']['manager_policy']\n",
        "            prob_correct_goal = result['custom_metrics']['prob_correct_goal_mean']\n",
        "\n",
        "            manager_mean_rewards.append(manager_mean_reward)\n",
        "            prob_correct_goal_means.append(prob_correct_goal)\n",
        "\n",
        "            print(pretty_print(result))\n",
        "            plt.plot(manager_mean_rewards)\n",
        "            plt.show()\n",
        "            \n",
        "            checkpoint_counter += 1\n",
        "            # save a checkpoint every checkpoint_interval trains\n",
        "            if(checkpoint_counter == checkpoint_interval):\n",
        "                trainer.save(checkpoint_dir)\n",
        "                info_logger.info(\"Checkpoint saved ({m}: iteration {n})\".format(m=mode, n=result['training_iteration']))\n",
        "                checkpoint_counter = 0\n",
        "                # write the checkpoint to epoch dict \n",
        "                epoch_by_checkpoint[mode][str(result['training_iteration'])] = curr_epoch\n",
        "                write_epoch_by_checkpoint(epoch_by_checkpoint)\n",
        "\n",
        "            # update the log \n",
        "            training_data = []\n",
        "            training_data.append(elapsed_episodes) # first entry is the total number of episodes\n",
        "            training_data.append(episode_mean_reward) # second entry is the mean episode length\n",
        "            training_data += [result['policy_reward_mean'][policy_name] \n",
        "                                   for policy_name in policies.keys()]  # other entries are mean policy rewards\n",
        "            training_data.append(prob_correct_goal)\n",
        "            training_data_df = pd.DataFrame([training_data], columns=savedata_columns)\n",
        "            savedata = savedata.append(training_data_df, ignore_index=True)\n",
        "\n",
        "        # print results on file\n",
        "        savedata.to_csv(savedata_file_path)\n",
        "\n",
        "        # compute epoch's results\n",
        "        curr_epoch_mean_reward = np.mean(manager_mean_rewards)\n",
        "        curr_epoch_prob_correct_goal = np.mean(prob_correct_goal_means)\n",
        "        \n",
        "        results_logger.info(\"Epoch: {}\".format(curr_epoch))\n",
        "        results_logger.info(\"\\tmean reward = {}\".format(curr_epoch_mean_reward))\n",
        "        results_logger.info(\"\\tprobability of correct goal = {}\".format(curr_epoch_prob_correct_goal))\n",
        "\n",
        "        epoch_mean_rewards.append(curr_epoch_mean_reward)        \n",
        "\n",
        "        # check convergence conditions\n",
        "        if curr_epoch > pretraining_n_epochs + window_size:\n",
        "\n",
        "            window_reward = 0\n",
        "            for r in epoch_mean_rewards[-5:]:\n",
        "                window_reward += r\n",
        "\n",
        "            if abs(curr_epoch_mean_reward - window_reward) / window_reward <= min_rel_delta_reward:\n",
        "                convergence_counter += 1\n",
        "                if convergence_counter >= 5 and curr_epoch <= training_n_epochs - 10:\n",
        "                    convergence = True\n",
        "            else:\n",
        "                convergence = False\n",
        "                convergence_counter = 0 \n",
        "            \n",
        "        curr_epoch +=1\n",
        "\n",
        "    if convergence:\n",
        "        results_logger.info(\"Convergence! The mean reward has remained stable for {} epochs\".format(convergence_counter))\n",
        "    elif convergence_counter > 0:\n",
        "        results_logger.info(\"No convergence. The mean reward stabilized for the first time around epoch {}\".format(1 + training_n_epochs - convergence_counter))\n",
        "    else:\n",
        "        results_logger.info(\"No convergence. The mean reward has never stabilized.\")\n",
        "\n",
        "finally:\n",
        "    ray.shutdown()"
      ],
      "id": "collective-therapy",
      "execution_count": null,
      "outputs": []
    }
  ]
}