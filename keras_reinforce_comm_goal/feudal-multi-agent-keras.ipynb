{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"feudal-multi-agent-keras.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hZgLY6rMHGN3"},"source":["# # If you are running on Google Colab, please install TensorFlow 2.0 by uncommenting below..\n","\n","# try:\n","#   # %tensorflow_version only exists in Colab.\n","#   %tensorflow_version 2.x\n","# except Exception:\n","#   pass\n","\n","# # If you are running on Google Colab, uncomment below to install the necessary dependencies \n","# # before beginning the exercise.\n","\n","# print(\"Setting up colab environment\")\n","# !pip install lz4\n","# !pip install gputil\n","# !pip uninstall -y -q pyarrow\n","# !pip install -q -U ray[tune]\n","# !pip install -q ray[debug]\n","\n","# # A hack to force the runtime to restart, needed to include the above dependencies.\n","# print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n","# import os\n","# os._exit(0)"],"id":"hZgLY6rMHGN3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNh216CCm9ID"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"JNh216CCm9ID","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BPkgcG3paKwS"},"source":["import os\n","os.chdir(\"/content/drive/MyDrive/semester_project_experiments/keras_reinforce_comm_goal\")"],"id":"BPkgcG3paKwS","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sporting-dover"},"source":["import pickle\n","import numpy as np\n","from logger import info_logger, results_logger\n","from envs.particle_rllib.environment import ParticleEnv\n","from reinforce_agent import ReinforceAgent"],"id":"sporting-dover","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-OYLh26XyMB"},"source":["import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = 12, 8"],"id":"o-OYLh26XyMB","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8qG4UBexSNm"},"source":["## Helpers"],"id":"x8qG4UBexSNm"},{"cell_type":"code","metadata":{"id":"extra-image"},"source":["# Function that creates the environment\n","def create_env_fn():\n","    return ParticleEnv(n_listeners=n_listeners, \n","                       n_landmarks=n_landmarks,\n","                       render_enable=render_enable)\n","    \n","class Results(dict):\n","    \n","    def __init__(self, *args, **kwargs):\n","        if 'filename' in kwargs:\n","            data = np.load(kwargs['filename'])\n","            super().__init__(data)\n","        else:\n","            super().__init__(*args, **kwargs)\n","        self.new_key = None\n","        self.plot_keys = None\n","        self.ylim = None\n","        \n","    def __setitem__(self, key, value):\n","        super().__setitem__(key, value)\n","        self.new_key = key\n","\n","    def plot(self, window):\n","        clear_output(wait=True)\n","        for key in self:\n","            #Ensure latest results are plotted on top\n","            if self.plot_keys is not None and key not in self.plot_keys:\n","                continue\n","            elif key == self.new_key:\n","                continue\n","            self.plot_smooth(key, window)\n","        if self.new_key is not None:\n","            self.plot_smooth(self.new_key, window)\n","        plt.xlabel('Episode')\n","        plt.ylabel('Reward')\n","        plt.legend(loc='lower right')\n","        if self.ylim is not None:\n","            plt.ylim(self.ylim)\n","        plt.show()\n","        \n","    def plot_smooth(self, key, window):\n","        if len(self[key]) == 0:\n","            plt.plot([], [], label=key)\n","            return None\n","        y = np.convolve(self[key], np.ones((window,))/window, mode='valid')\n","        x = np.linspace(window/2, len(self[key]) - window/2, len(y))\n","        plt.plot(x, y, label=key)\n","        \n","    def save(self, filename='results'):\n","        results_dir = 'results/'\n","        if not os.path.exists(results_dir):\n","            os.makedirs(results_dir)\n","        np.savez(results_dir + filename, **self)"],"id":"extra-image","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"retired-executive"},"source":["## Parameters"],"id":"retired-executive"},{"cell_type":"code","metadata":{"id":"boring-giant"},"source":["# pretraining parameters\n","pretraining_n_epochs = 10\n","pretraining = False\n","\n","# training parameters\n","training_n_epochs = 100\n","\n","# common parameters\n","n_episodes = 1000 # number of episodes in one epoch\n","n_steps = 25 # number of steps in one episode\n","policy_learning_rate = 0.002 \n","value_learning_rate = 0.01 # for updating the target network\n","gamma = 0.75 # discount factor\n","n_layers = 3\n","n_neurons = 128\n","\n","# environment config parameters\n","n_listeners = 1 \n","n_landmarks = 12\n","render_enable = False\n","\n","# convergence parameters\n","window_size = 5 # size of the sliding window \n","min_rel_delta_reward = 0.02  # minimum acceptable variation of the reward"],"id":"boring-giant","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOJbJTdmRBZm"},"source":["## Initialize the environment and the agents"],"id":"KOJbJTdmRBZm"},{"cell_type":"code","metadata":{"id":"4q_xbbQGQ1Kx"},"source":["env = create_env_fn()\n","\n","# According to environment implementation, there exists a different action space and observation space for each agent, \n","# action_space[0] (resp. observations_space[0]) is allocated for the manager, while the others are allocated for the workers\n","manager_action_space = env.action_space[0]\n","manager_observation_space = env.observation_space[0]\n","worker_action_space = env.action_space[1]\n","worker_observation_space = env.observation_space[1]\n","\n","# Initiate the manager\n","manager = ReinforceAgent(name='manager',\n","                n_obs=manager_observation_space.shape[0], \n","                action_space=manager_action_space,\n","                policy_learning_rate=policy_learning_rate, \n","                value_learning_rate=value_learning_rate, \n","                discount=gamma, \n","                n_layers=n_layers,\n","                n_neurons=n_neurons)\n","\n","info_logger.info(\"Manager agent initialized\")\n"," \n","# Initiate the listener\n","worker = ReinforceAgent(name='worker',\n","                n_obs=worker_observation_space.shape[0], \n","                action_space=worker_action_space,\n","                policy_learning_rate=policy_learning_rate, \n","                value_learning_rate=value_learning_rate, \n","                discount=gamma, \n","                n_layers=n_layers,\n","                n_neurons=n_neurons)\n","\n","info_logger.info(\"Worker agent initialized\")"],"id":"4q_xbbQGQ1Kx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzbfPMuIcx2u"},"source":["def run_experiment(env, is_training, n_steps, n_episodes, past_worker_rewards, past_manager_rewards):\n","\n","    manager_rewards = []\n","    worker_rewards = []\n","    probs_correct_goal = []\n","\n","    for episode in range(1, n_episodes+1):\n","\n","        if episode % 10 == 0:\n","            results['worker'] = np.array(train_worker_rewards + worker_rewards)\n","            results['manager'] = np.array(train_manager_rewards + manager_rewards)\n","            results.plot(10)\n","\n","        #Reset the environment to a new episode\n","        obs = env.reset()\n","        ext_comm_reward = 0\n","        episode_manager_reward = 0\n","        episode_worker_reward = 0\n","        step = 1\n","        correct_goals = []\n","        action = {}\n","\n","        while True:\n","\n","            # 1. Decide on an action based on the observations\n","            action['worker_agent_1'] = worker.decide(obs['worker_agent_1'])\n","            if 'manager_agent' in obs:\n","                # if no observation for the manager, then extend communication\n","                action['manager_agent'] = manager.decide(obs['manager_agent'])\n","\n","            # 2. Take action in the environment\n","            next_obs, rewards, done, _ = env.step(action)\n","            ext_comm_reward += rewards['manager_agent']\n","            episode_worker_reward += rewards['worker_agent_1']\n","            episode_manager_reward += rewards['manager_agent']\n","\n","            # 3. Store the information returned from the environment for training\n","            worker.observe(obs['worker_agent_1'], action['worker_agent_1'], rewards['worker_agent_1'])\n","            if is_training and 'manager_agent' in obs:\n","                if step > 1:\n","                    manager.observe(manager_obs, manager_action, ext_comm_reward)\n","                    ext_comm_reward = 0\n","                manager_obs = obs['manager_agent']\n","                manager_action = action['worker_agent_1']\n","                \n","            # 4. When we reach a terminal state (\"done\"), use the observed episode to train the network\n","            if step == n_steps:\n","                manager_rewards.append(episode_manager_reward)                \n","                worker_rewards.append(episode_worker_reward)\n","                worker.train()\n","                if is_training:\n","                    manager.train()\n","                break\n","\n","            # Reset for next step\n","            obs = next_obs\n","            step += 1\n","    \n","    return manager_rewards, worker_rewards"],"id":"TzbfPMuIcx2u","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"indonesian-horizon"},"source":["## Simulation loop"],"id":"indonesian-horizon"},{"cell_type":"code","metadata":{"id":"collective-therapy"},"source":["convergence = False\n","convergence_counter = 0 \n","train_manager_rewards = []\n","train_worker_rewards = []\n","epoch_mean_rewards = [] # mean reward of the epoch in time\n","curr_epoch = 1\n","results = Results()\n","\n","while curr_epoch <= training_n_epochs:\n","\n","    # loop for training_n_epochs\n","    \n","    info_logger.info(\"Current epoch: {}\".format(curr_epoch))\n","\n","    if pretraining and curr_epoch <= pretraining_n_epochs:\n","        manager_rewards, worker_rewards = run_experiment(\n","            env, \n","            is_training=False, \n","            n_steps=n_steps, \n","            n_episodes=n_episodes,\n","            past_worker_rewards=train_worker_rewards,\n","            past_manager_rewards=train_manager_rewards\n","        )\n","    else:\n","        manager_rewards, worker_rewards = run_experiment(\n","                env, \n","                is_training=True, \n","                n_steps=n_steps, \n","                n_episodes=n_episodes,\n","                past_worker_rewards=train_worker_rewards,\n","                past_manager_rewards=train_manager_rewards\n","            )\n","\n","    train_worker_rewards += worker_rewards\n","    train_manager_rewards += manager_rewards\n","    results.save(filename='results-ep{}'.format(curr_epoch))\n","    \n","    curr_epoch_mean_manager_reward = np.mean(manager_rewards)\n","    curr_epoch_mean_worker_reward = np.mean(worker_rewards)\n","\n","    results_logger.info(\"Epoch: {}\".format(curr_epoch))\n","    results_logger.info(\"\\tmanager mean reward = {}\".format(curr_epoch_mean_manager_reward))\n","    results_logger.info(\"\\tworker mean reward = {}\".format(curr_epoch_mean_worker_reward))\n","\n","    epoch_mean_rewards.append(curr_epoch_mean_manager_reward)  \n","\n","    # check convergence conditions\n","    if curr_epoch > pretraining_n_epochs + window_size:\n","\n","        window_reward = 0\n","        for r in epoch_mean_rewards[-5:]:\n","            window_reward += r\n","\n","        if abs(curr_epoch_mean_manager_reward - window_reward) / window_reward <= min_rel_delta_reward:\n","            convergence_counter += 1\n","            if convergence_counter >= 5 and curr_epoch <= training_n_epochs - 10:\n","                convergence = True\n","        else:\n","            convergence = False\n","            convergence_counter = 0 \n","        \n","    curr_epoch +=1\n","\n","manager.save()\n","worker.save()\n","\n","if convergence:\n","    results_logger.info(\"Convergence! The mean reward has remained stable for {} epochs\".format(convergence_counter))\n","elif convergence_counter > 0:\n","    results_logger.info(\"No convergence. The mean reward stabilized for the first time around epoch {}\".format(1 + training_n_epochs - convergence_counter))\n","else:\n","    results_logger.info(\"No convergence. The mean reward has never stabilized.\")"],"id":"collective-therapy","execution_count":null,"outputs":[]}]}