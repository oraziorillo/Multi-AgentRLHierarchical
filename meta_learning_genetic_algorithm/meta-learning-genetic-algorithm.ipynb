{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"meta-learning-genetic-algorithm.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"JNh216CCm9ID"},"source":["# If you are running on Google Colab, please mount the drive uncommenting below\n","\n","import os\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","os.chdir(\"/content/drive/MyDrive/semester_project_experiments/meta_learning_genetic_algorithm/\")"],"id":"JNh216CCm9ID","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZgLY6rMHGN3"},"source":["# # If you are running on Google Colab, please install TensorFlow 2.0 by uncommenting below..\n","\n","# try:\n","#   # %tensorflow_version only exists in Colab.\n","#   %tensorflow_version 2.x\n","# except Exception:\n","#   pass\n","\n","# # If you are running on Google Colab, uncomment below to install the necessary dependencies \n","# # before beginning the exercise.\n","\n","# print(\"Setting up colab environment\")\n","# !pip uninstall -y -q pyarrow\n","# !pip install -q -U ray[tune]\n","# !pip install -q ray[debug]\n","# !pip install lz4\n","# !pip install gputil\n","\n","# # A hack to force the runtime to restart, needed to include the above dependencies.\n","# print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n","# import os\n","# os._exit(0)"],"id":"hZgLY6rMHGN3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sporting-dover"},"source":["import json\n","import pickle\n","import datetime\n","import pprint\n","import logging\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import ray\n","import genetic as ga\n","from copy import deepcopy\n","from ray import tune\n","from IPython.display import clear_output \n","from ray.tune.registry import register_env\n","from ray.tune.logger import pretty_print\n","from ray.rllib.agents.ppo.ppo import PPOTrainer\n","from envs.particle_rllib.environment import ParticleEnv\n","from logger import info_logger, results_logger"],"id":"sporting-dover","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8qG4UBexSNm"},"source":["## Helper functions"],"id":"x8qG4UBexSNm"},{"cell_type":"code","metadata":{"id":"extra-image"},"source":["# Function that creates the environment\n","def create_env_fn(env_context=None):\n","    return ParticleEnv(n_listeners=n_listeners, \n","                       n_landmarks=n_landmarks,\n","                       render_enable=render_enable)\n","\n","# Function that maps a policy to its agent id\n","def policy_mapping_fn(agent_id):\n","    if agent_id.startswith('manager'):\n","        return \"manager_policy\"\n","    else:\n","        return \"worker_policy\""],"id":"extra-image","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"retired-executive"},"source":["## Parameters"],"id":"retired-executive"},{"cell_type":"code","metadata":{"id":"boring-giant"},"source":["# genetic algorithm parameters\n","n_pop=250\n","r_cross=0.9\n","r_mut=0.9\n","\n","# training parameters\n","training_algo = \"PPO\"\n","env_name = \"ParticleManagerListeners\"\n","n_epochs = 10\n","n_episodes = 3000 # number of episodes in one epoch\n","n_steps = 25 # number of steps in one episode\n","learning_rate = 5e-4\n","tau = 0.01 # for updating the target network\n","gamma = 0.75 # discount factor\n","replay_buffer_size = 10000000\n","batch_size = 1024\n","hidden_layers = [16, 16]\n","\n","# environment config parameters\n","n_listeners = 1 \n","n_landmarks = 12\n","render_enable = False\n","\n","# convergence parameters\n","window_size = 5 # size of the sliding window \n","min_rel_delta_reward = 0.02  # minimum acceptable variation of the reward\n","\n","savedata_dir = './savedata/' # savedata directory\n","checkpoint_dir = './checkpoints/' # checkpoints directory\n","restore_checkpoint_n = 10 \n","\n","# Create savedata directory\n","if not os.path.exists(savedata_dir):\n","    os.makedirs(savedata_dir)\n","\n","# Create the checkpoint directory\n","if not os.path.exists(checkpoint_dir):\n","    os.makedirs(checkpoint_dir)"],"id":"boring-giant","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOJbJTdmRBZm"},"source":["## Trainers configuration"],"id":"KOJbJTdmRBZm"},{"cell_type":"code","metadata":{"id":"unavailable-boundary"},"source":["env = create_env_fn()\n","\n","# According to environment implementation, there exists a different action space and observation space for each agent, \n","# action_space[0] (resp. observations_space[0]) is allocated for the manager, while the others are allocated for the workers\n","manager_action_space = env.action_space[0]\n","manager_observation_space = env.observation_space[0]\n","worker_action_space = env.action_space[1]\n","worker_observation_space = env.observation_space[1]\n","\n","policies = {\n","    \"manager_policy\": (None, manager_observation_space, manager_action_space, {\"lr\": 0.0,}),\n","    \"worker_policy\": (None, worker_observation_space, worker_action_space, {\"lr\": learning_rate,})\n","    }\n","\n","training_config = {\n","    \"num_workers\": 8,\n","    # \"lr\": learning_rate,\n","    # \"tau\": tau, \n","    \"gamma\": gamma,\n","    \"horizon\": n_steps,\n","    # \"actor_hiddens\": hidden_layers,\n","    # \"critic_hiddens\": hidden_layers,\n","    # \"buffer_size\": replay_buffer_size, #---> too much memory required\n","    \"train_batch_size\": batch_size,\n","    \"model\": {\n","        \"fcnet_hiddens\": hidden_layers\n","    },\n","    \"multiagent\": {\n","        \"policies\": policies,\n","        \"policy_mapping_fn\": policy_mapping_fn,\n","        \"policies_to_train\": list(policies.keys())\n","    },\n","    \"no_done_at_end\": True,\n","    \"log_level\": \"ERROR\"\n","}\n","\n","# Initialize and register the environment\n","register_env(env_name, create_env_fn)"],"id":"unavailable-boundary","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XcSf2hxbwIW-"},"source":["def objective(i, individual):\n","\n","    print(\"Starting evaluation of the individual {}\".format(i+1))\n","\n","    elapsed_episodes = 0\n","    manager_total_reward = 0\n","    \n","    register_env(env_name, create_env_fn)\n","\n","    trainer = PPOTrainer(env=env_name, config=training_config)\n","    weights = trainer.get_weights()\n","    weights['manager_policy'] = ga.convert_individual_to_manager_weights(individual, weights['manager_policy'])\n","    trainer.set_weights(weights)\n","\n","    # Loop for n_episodes\n","    while elapsed_episodes < n_episodes:\n","        result = trainer.train()\n","        elapsed_episodes = result['episodes_total']\n","        manager_total_reward += (result['policy_reward_mean']['manager_policy'] * result['episodes_this_iter'])\n","        print(pretty_print(result))\n","    \n","    trainer.stop()\n","    clear_output()\n","    return manager_total_reward"],"id":"XcSf2hxbwIW-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJOOoITMwNiv"},"source":["def genetic_algorithm(example, n_gen, n_pop, r_cross, r_mut, restore=False):\n","\n","    if restore:\n","        with open(checkpoint_dir + \"population\", 'rb') as fp:\n","            pop = pickle.load(fp)\n","        with open(savedata_dir + \"epoch-manager-rewards\", 'rb') as fp:\n","            epoch_manager_rewards = pickle.load(fp)\n","        with open(checkpoint_dir + \"best-individual\", 'rb') as fp:\n","            best = pickle.load(fp)\n","        best_eval = epoch_manager_rewards[-1]\n","        gen = len(epoch_manager_rewards) + 1\n","            \n","    else:\n","        # initial population of random bitstring\n","        pop = [ga.generate_random_individual(example=example) for _ in range(n_pop)]\n","        # keep track of best solution\n","        best, best_eval = 0, objective(-1, pop[0])\n","        # best total reward of the manager per each epoch\n","        epoch_manager_rewards = [] \n","        gen = 1\n","\n","    # enumerate generations\n","    while gen <= n_gen:\n","        \n","        info_logger.info(\"Current generation: {}\".format(gen))\n","\n","        # evaluate all candidates in the population\n","        scores = [objective(i, c) for (i, c) in enumerate(pop)]\n","\n","        # check for new best solution\n","        for i in range(n_pop):\n","            if scores[i] > best_eval:\n","                best, best_eval = pop[i], scores[i]\n","\n","        results_logger.info(\"Generation: {}\".format(gen))\n","        results_logger.info(\"\\tbest score = {:.3f}\".format(best_eval))\n","\n","        # save checkpoint \n","        with open(checkpoint_dir + \"best-individual\".format(gen), 'wb') as fp:\n","            pickle.dump(best, fp)\n","        info_logger.info(\"Saved checkpoint after the evaluation of the generation {}\".format(gen))\n","\n","        epoch_manager_rewards.append(best_eval)   \n","        with open(savedata_dir + \"epoch-manager-rewards\", 'wb') as fp:\n","            pickle.dump(epoch_manager_rewards, fp)\n","\n","        plt.plot(epoch_manager_rewards)   \n","        plt.xlabel('Generation')\n","        plt.ylabel('Reward') \n","        plt.show()\n","\n","        # select parents\n","        selected = [ga.selection(pop, scores, k=(n_pop//10)) for _ in range(n_pop)]\n","\n","        # create the next generation\n","        children = list()\n","\n","        for i in range(0, n_pop, 2):\n","\n","            # get selected parents in pairs\n","            p1, p2 = selected[i], selected[i+1]\n","\n","            # crossover and mutation\n","            for c in ga.crossover(p1, p2, r_cross):\n","                ga.mutation(c, r_mut) # mutation\n","                children.append(c) # store for next generation\n","\n","        # replace population\n","        pop = children\n","\n","        with open(checkpoint_dir + \"population\", 'wb') as fp:\n","            pickle.dump(pop, fp)\n","        \n","        gen += 1\n","\n","    return [best, best_eval]"],"id":"mJOOoITMwNiv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ExfWsqXs21OU"},"source":["ray.init()\n","\n","trainer = PPOTrainer(env=env_name, config=training_config)\n","\n","# Print the current configuration\n","pp = pprint.PrettyPrinter(indent=4)\n","print(\"Current configiguration\\n-----------------------\")\n","pp.pprint(trainer.get_config())\n","print(\"-----------------------\\n\")\n","\n","manager_weights_ex = trainer.get_weights()['manager_policy']\n","trainer.stop()\n","\n","best, best_eval = genetic_algorithm(example=manager_weights_ex,\n","                                    n_gen=n_epochs,\n","                                    n_pop=n_pop,\n","                                    r_cross=r_cross,\n","                                    r_mut=r_mut,\n","                                    restore=False)\n","\n","ray.shutdown()\n"],"id":"ExfWsqXs21OU","execution_count":null,"outputs":[]}]}