{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"meta-learning-random-search.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"1WpmZJ1x1Qw8"},"source":["# If you are running on Google Colab, please mount the drive uncommenting below\n","\n","import os\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","os.chdir(\"/content/drive/MyDrive/semester_project_experiments/meta_learning_random_search/\")"],"id":"1WpmZJ1x1Qw8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FGUbQQ9y1QfT"},"source":["# # If you are running on Google Colab, please install TensorFlow 2.0 by uncommenting below..\n","\n","# try:\n","#   # %tensorflow_version only exists in Colab.\n","#   %tensorflow_version 2.x\n","# except Exception:\n","#   pass\n","\n","# # If you are running on Google Colab, uncomment below to install the necessary dependencies \n","# # before beginning the exercise.\n","\n","# print(\"Setting up colab environment\")\n","# !pip uninstall -y -q pyarrow\n","# !pip install -q -U ray[tune]\n","# !pip install -q ray[debug]\n","# !pip install lz4\n","# !pip install gputil\n","\n","# # A hack to force the runtime to restart, needed to include the above dependencies.\n","# print(\"Done installing! Restarting via forced crash (this is not an issue).\")\n","# import os\n","# os._exit(0)"],"id":"FGUbQQ9y1QfT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sporting-dover"},"source":["import json\n","import pickle\n","import datetime\n","import pprint\n","import logging\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from copy import deepcopy\n","import ray\n","from ray import tune\n","from ray.tune.registry import register_env\n","from ray.tune.logger import pretty_print\n","from ray.rllib.agents.ppo.ppo import PPOTrainer\n","from envs.particle_rllib.environment import ParticleEnv\n","from logger import info_logger, results_logger"],"id":"sporting-dover","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8qG4UBexSNm"},"source":["## Helper functions"],"id":"x8qG4UBexSNm"},{"cell_type":"code","metadata":{"id":"extra-image"},"source":["# Function that creates the environment\n","def create_env_fn(env_context=None):\n","    return ParticleEnv(n_listeners=n_listeners, \n","                       n_landmarks=n_landmarks,\n","                       render_enable=render_enable)\n","\n","# Function that maps a policy to its agent id\n","def policy_mapping_fn(agent_id):\n","    if agent_id.startswith('manager'):\n","        return \"manager_policy\"\n","    else:\n","        return \"worker_policy\""],"id":"extra-image","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3amRfZc41dIv"},"source":["def sample_position_on_hypersphere(initial_pos):\n","    arr_lenght = len(initial_pos)\n","    x = np.random.normal(size=arr_lenght)\n","    radius = np.sqrt(np.sum(np.square(x)))\n","    return np.add(initial_pos, np.divide(x, radius))\n","    \n","def sample_neighboring_weights(old_weights):\n","    new_weights = deepcopy(old_weights)\n","\n","    new_weights['manager_policy/fc_1/kernel'][0] = sample_position_on_hypersphere(old_weights['manager_policy/fc_1/kernel'][0])\n","    new_weights['manager_policy/fc_1/bias'] = sample_position_on_hypersphere(old_weights['manager_policy/fc_1/bias'])\n","\n","    new_weights['manager_policy/fc_value_1/kernel'][0] = sample_position_on_hypersphere(old_weights['manager_policy/fc_value_1/kernel'][0])\n","    new_weights['manager_policy/fc_value_1/bias'] = sample_position_on_hypersphere(old_weights['manager_policy/fc_value_1/bias'])\n","\n","    for i in range(len(old_weights['manager_policy/fc_2/kernel'])):\n","        new_weights['manager_policy/fc_2/kernel'][i] = sample_position_on_hypersphere(old_weights['manager_policy/fc_2/kernel'][i])\n","    new_weights['manager_policy/fc_2/bias'] = sample_position_on_hypersphere(old_weights['manager_policy/fc_2/bias'])\n","\n","    for i in range(len(old_weights['manager_policy/fc_value_2/kernel'])):\n","        new_weights['manager_policy/fc_value_2/kernel'][i] = sample_position_on_hypersphere(old_weights['manager_policy/fc_value_2/kernel'][i])\n","    new_weights['manager_policy/fc_value_2/bias'] = sample_position_on_hypersphere(old_weights['manager_policy/fc_value_2/bias'])\n","\n","    for i in range(len(old_manager_weights['manager_policy/fc_out/kernel'][0])):\n","        new_weights['manager_policy/fc_out/kernel'][:, i] = sample_position_on_hypersphere(old_weights['manager_policy/fc_out/kernel'][:, i])\n","    new_weights['manager_policy/fc_out/bias'] = sample_position_on_hypersphere(old_weights['manager_policy/fc_out/bias'])\n","\n","    new_weights['manager_policy/value_out/kernel'][:, 0] = sample_position_on_hypersphere(old_weights['manager_policy/value_out/kernel'][:, 0])\n","    new_weights['manager_policy/value_out/bias'] = sample_position_on_hypersphere(old_weights['manager_policy/value_out/bias'])\n","\n","    return new_weights"],"id":"3amRfZc41dIv","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"retired-executive"},"source":["## Parameters"],"id":"retired-executive"},{"cell_type":"code","metadata":{"id":"boring-giant"},"source":["# training parameters\n","training_algo = \"PPO\"\n","env_name = \"ParticleManagerListeners\"\n","n_epochs = 100\n","n_episodes = 5000 # number of episodes in one epoch\n","n_steps = 25 # number of steps in one episode\n","learning_rate = 5e-4\n","tau = 0.01 # for updating the target network\n","gamma = 0.75 # discount factor\n","replay_buffer_size = 10000000\n","batch_size = 1024\n","hidden_layers = [256, 256]\n","\n","# environment config parameters\n","n_listeners = 1 \n","n_landmarks = 12\n","render_enable = False\n","\n","# convergence parameters\n","window_size = 5 # size of the sliding window \n","min_rel_delta_reward = 0.02  # minimum acceptable variation of the reward\n","\n","savedata_dir = './savedata/' # savedata directory\n","checkpoint_dir = './checkpoints/' # checkpoints directory\n","restore_checkpoint_n = 0 \n","\n","# Create savedata directory\n","if not os.path.exists(savedata_dir):\n","    os.makedirs(savedata_dir)\n","\n","# Create the checkpoint directory\n","if not os.path.exists(checkpoint_dir):\n","    os.makedirs(checkpoint_dir)"],"id":"boring-giant","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOJbJTdmRBZm"},"source":["## Trainers configuration"],"id":"KOJbJTdmRBZm"},{"cell_type":"code","metadata":{"id":"unavailable-boundary"},"source":["env = create_env_fn()\n","\n","# According to environment implementation, there exists a different action space and observation space for each agent, \n","# action_space[0] (resp. observations_space[0]) is allocated for the manager, while the others are allocated for the workers\n","manager_action_space = env.action_space[0]\n","manager_observation_space = env.observation_space[0]\n","worker_action_space = env.action_space[1]\n","worker_observation_space = env.observation_space[1]\n","\n","policies = {\n","    \"manager_policy\": (None, manager_observation_space, manager_action_space, {\"lr\": 0.0,}),\n","    \"worker_policy\": (None, worker_observation_space, worker_action_space, {\"lr\": learning_rate,})\n","    }\n","\n","training_config = {\n","    \"num_workers\": 8,\n","    # \"lr\": learning_rate,\n","    # \"tau\": tau, \n","    \"gamma\": gamma,\n","    \"horizon\": n_steps,\n","    # \"actor_hiddens\": hidden_layers,\n","    # \"critic_hiddens\": hidden_layers,\n","    # \"buffer_size\": replay_buffer_size, #---> too much memory required\n","    \"train_batch_size\": batch_size,\n","    \"model\": {\n","        \"fcnet_hiddens\": [16, 16]\n","    },\n","    \"multiagent\": {\n","        \"policies\": policies,\n","        \"policy_mapping_fn\": policy_mapping_fn,\n","        \"policies_to_train\": list(policies.keys())\n","    },\n","    \"no_done_at_end\": True,\n","    \"log_level\": \"ERROR\"\n","}\n","\n","# Initialize and register the environment\n","register_env(env_name, create_env_fn)"],"id":"unavailable-boundary","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"collective-therapy"},"source":["# Initialize Ray\n","ray.init()\n","trainer = PPOTrainer(env=env_name, config=training_config)\n","# Print the current configuration\n","pp = pprint.PrettyPrinter(indent=4)\n","print(\"Current configiguration\\n-----------------------\")\n","pp.pprint(trainer.get_config())\n","print(\"-----------------------\\n\")\n","\n","if restore_checkpoint_n != 0:\n","    # restore the old weights\n","    weights = trainer.get_weights()\n","    with open(checkpoint_dir + \"manager_weights_{}\".format(restore_checkpoint_n), 'rb') as fp:\n","        old_manager_weights = pickle.load(fp)\n","    weights['manager_policy'] = old_manager_weights\n","    trainer.set_weights(weights)\n","    info_logger.info(\"Restored checkpoint from epoch {}\".format(restore_checkpoint_n))\n","    # restore the history of manager rewads\n","    with open(savedata_dir + \"epoch-manager-rewards\", 'rb') as fp:\n","        epoch_manager_rewards = pickle.load(fp)\n","    # restore the correct number of epoch \n","    curr_epoch = len(epoch_manager_rewards) + 1\n","    # restore the old best manager reward\n","    old_manager_reward = epoch_manager_rewards[restore_checkpoint_n - 1]\n","\n","else:\n","    curr_epoch = 1\n","    info_logger.info(\"Initializing with training mode\")\n","    old_manager_reward = np.NINF\n","    old_manager_weights = trainer.get_weights()['manager_policy']\n","    epoch_manager_rewards = [] # total reward of the manager in each epoch\n","\n","# Init simulation variables\n","convergence = False\n","convergence_counter = 0 \n","\n","while curr_epoch <= n_epochs:\n","\n","    # loop for n_epochs\n","    \n","    info_logger.info(\"Current epoch: {}\".format(curr_epoch))\n","\n","    manager_total_reward = 0 # total rewards of the manager in this epoch\n","\n","    # initialize iteration data saving log\n","    savedata_file_name = '{}-epoch.csv'.format(curr_epoch)\n","    savedata_file_path = savedata_dir + savedata_file_name\n","    savedata_columns = [\"episodes_total\", \"episode_mean_reward\", \"worker_mean_reward\", \"manager_mean_reward\", \"manager_total_reward\"]\n","    savedata = pd.DataFrame(columns=savedata_columns)\n","\n","    elapsed_episodes = 0\n","\n","    # Loop for n_episodes\n","    while elapsed_episodes < n_episodes:\n","\n","        result = trainer.train()\n","\n","        elapsed_episodes = result['episodes_total']\n","        episode_mean_reward = result['episode_reward_mean']\n","        manager_mean_reward = result['policy_reward_mean']['manager_policy']\n","        worker_mean_reward = result['policy_reward_mean']['worker_policy']\n","\n","        manager_total_reward += (manager_mean_reward * result['episodes_this_iter'])\n","\n","        # update the log \n","        training_data = []\n","        training_data.append(elapsed_episodes) # first entry is the total number of episodes\n","        training_data.append(episode_mean_reward) # second entry is the mean episode reward\n","        training_data.append(worker_mean_reward) # third entry is the worker mean reward\n","        training_data.append(manager_mean_reward) # fourth entry is the manager mean reward\n","        training_data.append(manager_total_reward) # fifth entry is the manager total reward up to now\n","        training_data_df = pd.DataFrame([training_data], columns=savedata_columns)\n","        savedata = savedata.append(training_data_df, ignore_index=True)\n","\n","        print(pretty_print(result))\n","\n","    # update the weights of the manager if the new set of weights is better than the previous one\n","    if manager_total_reward >= old_manager_reward:\n","        old_manager_reward = manager_total_reward\n","        old_manager_weights = trainer.get_weights()['manager_policy']\n","        # save checkpoint \n","        with open(checkpoint_dir + \"manager_weights_{}\".format(curr_epoch), 'wb') as fp:\n","            pickle.dump(old_manager_weights, fp)\n","        info_logger.info(\"Saved checkpoint in epoch {}\".format(curr_epoch))\n","\n","    trainer.stop()\n","    trainer = PPOTrainer(env=env_name, config=training_config)\n","    weights = trainer.get_weights()\n","    new_manager_weights = sample_neighboring_weights(old_manager_weights)\n","    weights['manager_policy'] = new_manager_weights\n","    trainer.set_weights(weights)\n","\n","    # print results on file\n","    savedata.to_csv(savedata_file_path)\n","    \n","    results_logger.info(\"Epoch: {}\".format(curr_epoch))\n","    results_logger.info(\"\\tmananger reward = {}\".format(manager_total_reward))\n","\n","    epoch_manager_rewards.append(manager_total_reward)   \n","    with open(savedata_dir + \"epoch-manager-rewards\", 'wb') as fp:\n","        pickle.dump(epoch_manager_rewards, fp)\n","\n","    plt.plot(epoch_manager_rewards)     \n","    plt.show()\n","\n","    # check convergence conditions\n","    if curr_epoch > n_epochs/2 + window_size:\n","\n","        window_reward = 0\n","        for r in epoch_manager_rewards[-5:]:\n","            window_reward += r\n","\n","        if abs(manager_total_reward - window_reward) / window_reward <= min_rel_delta_reward:\n","            convergence_counter += 1\n","            if convergence_counter >= 5 and curr_epoch <= n_epochs - 10:\n","                convergence = True\n","        else:\n","            convergence = False\n","            convergence_counter = 0 \n","        \n","    curr_epoch +=1\n","\n","# Save final checkpoint\n","trainer.stop()\n","ray.shutdown()\n","\n","if convergence:\n","    results_logger.info(\"Convergence! The mean reward has remained stable for {} epochs\".format(convergence_counter))\n","elif convergence_counter > 0:\n","    results_logger.info(\"No convergence. The mean reward stabilized for the first time around epoch {}\".format(1 + n_epochs - convergence_counter))\n","else:\n","    results_logger.info(\"No convergence. The mean reward has never stabilized.\")"],"id":"collective-therapy","execution_count":null,"outputs":[]}]}